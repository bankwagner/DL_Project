{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a85rjNpKe1oF",
        "outputId": "677e7a4a-7d01-4ae8-b642-3dd4422c8efd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File downloaded successfully.\n"
          ]
        }
      ],
      "source": [
        "# Download the dataset\n",
        "\n",
        "import requests\n",
        "\n",
        "# it takes ~15 mins to download\n",
        "url = \"https://humanheart-project.creatis.insa-lyon.fr/database/api/v1/collection/637218c173e9f0047faa00fb/download\"\n",
        "response = requests.get(url)\n",
        "\n",
        "if response.status_code == 200:\n",
        "    file_content = response.content\n",
        "    local_file_path = \"ACDC_temp.zip\"\n",
        "\n",
        "    with open(local_file_path, \"wb\") as local_file:\n",
        "        local_file.write(file_content)\n",
        "\n",
        "    print(\"File downloaded successfully.\")\n",
        "else:\n",
        "    print(\"Failed to download the file. Status code:\", response.status_code)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K2ncShnme1oH",
        "outputId": "2b0747b9-048f-437a-8f26-4de09bb45ac0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File unzipped successfully.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import zipfile\n",
        "\n",
        "# it takes ~30s to unzip\n",
        "current_directory = os.getcwd()\n",
        "zip_file_path = f'{current_directory}/ACDC_temp.zip'\n",
        "extracted_dir = f'{current_directory}'\n",
        "\n",
        "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extracted_dir)\n",
        "\n",
        "print(\"File unzipped successfully.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5SahQR-9e1oH",
        "outputId": "3b5cb844-65e8-4702-bda2-7cefcadf03b8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting dicom2nifti\n",
            "  Downloading dicom2nifti-2.4.8-py3-none-any.whl (43 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/43.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: nibabel in /usr/local/lib/python3.10/dist-packages (from dicom2nifti) (4.0.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from dicom2nifti) (1.23.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from dicom2nifti) (1.11.3)\n",
            "Collecting pydicom>=2.2.0 (from dicom2nifti)\n",
            "  Downloading pydicom-2.4.3-py3-none-any.whl (1.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m43.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting python-gdcm (from dicom2nifti)\n",
            "  Downloading python_gdcm-3.0.22-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.0/13.0 MB\u001b[0m \u001b[31m20.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from nibabel->dicom2nifti) (23.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from nibabel->dicom2nifti) (67.7.2)\n",
            "Installing collected packages: python-gdcm, pydicom, dicom2nifti\n",
            "Successfully installed dicom2nifti-2.4.8 pydicom-2.4.3 python-gdcm-3.0.22\n"
          ]
        }
      ],
      "source": [
        "# install packages\n",
        "!pip install dicom2nifti\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3wWRqdVwe1oH",
        "outputId": "c4799164-23bd-4b87-fa67-2cc3bb0f8ade"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nilearn\n",
            "  Downloading nilearn-0.10.2-py3-none-any.whl (10.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.4/10.4 MB\u001b[0m \u001b[31m26.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: joblib>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from nilearn) (1.3.2)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from nilearn) (4.9.3)\n",
            "Requirement already satisfied: nibabel>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from nilearn) (4.0.2)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from nilearn) (1.23.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from nilearn) (23.2)\n",
            "Requirement already satisfied: pandas>=1.1.5 in /usr/local/lib/python3.10/dist-packages (from nilearn) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.25.0 in /usr/local/lib/python3.10/dist-packages (from nilearn) (2.31.0)\n",
            "Requirement already satisfied: scikit-learn>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from nilearn) (1.2.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from nilearn) (1.11.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from nibabel>=3.2.0->nilearn) (67.7.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.5->nilearn) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.5->nilearn) (2023.3.post1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.25.0->nilearn) (3.3.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.25.0->nilearn) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.25.0->nilearn) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.25.0->nilearn) (2023.7.22)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.0.0->nilearn) (3.2.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas>=1.1.5->nilearn) (1.16.0)\n",
            "Installing collected packages: nilearn\n",
            "Successfully installed nilearn-0.10.2\n"
          ]
        }
      ],
      "source": [
        "!pip install nilearn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "6bNF9_mSe1oI"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Importing the necessary libraries\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "import dicom2nifti\n",
        "import nibabel as nib\n",
        "import nilearn as nil\n",
        "import nilearn.image as nili\n",
        "import scipy.ndimage as ndi\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import cv2\n",
        "from scipy.ndimage import rotate\n",
        "\n",
        "import skimage.transform as skt\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "os.environ[\"SM_FRAMEWORK\"] = \"tf.keras\"\n",
        "\n",
        "\"\"\"\n",
        "Read the DICOM files and convert them to NIFTI format\n",
        "Global variables:\n",
        "\n",
        "patient_vals_train      -> list of training values\n",
        "patient_vals_train_gt   -> list of training ground truth values\n",
        "patient_vals_test       -> list of testing values\n",
        "patient_vals_test_gt    -> list of testing ground truth values\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "patient_vols_train =[]\n",
        "patient_vols_train_gt =[]\n",
        "patient_vols_test =[]\n",
        "patient_vols_test_gt =[]\n",
        "\n",
        "\n",
        "original_affine = np.diag([-1, -1, 1, 1])\n",
        "\n",
        "\n",
        "def load_nifti_files():\n",
        "    for i in range(1, 101):\n",
        "        for frame_num in range(1, 17):\n",
        "            if os.path.isfile(f'ACDC/database/training/patient{101-i:03}/patient{101-i:03}_frame{frame_num:02}.nii.gz'):\n",
        "                patient_vols_train.append(nib.load(f'ACDC/database/training/patient{101-i:03}/patient{101-i:03}_frame{frame_num:02}.nii.gz'))\n",
        "            if os.path.isfile(f'ACDC/database/training/patient{101-i:03}/patient{101-i:03}_frame{frame_num:02}_gt.nii.gz'):\n",
        "                patient_vols_train_gt.append(nib.load(f'ACDC/database/training/patient{101-i:03}/patient{101-i:03}_frame{frame_num:02}_gt.nii.gz'))\n",
        "\n",
        "\n",
        "    for i in range(101, 151):\n",
        "        for frame_num in range(1, 17):\n",
        "            if os.path.isfile(f'ACDC/database/testing/patient{i}/patient{i}_frame{frame_num:02}.nii.gz'):\n",
        "                patient_vols_test.append(nib.load(f'ACDC/database/testing/patient{i}/patient{i}_frame{frame_num:02}.nii.gz'))\n",
        "            if os.path.isfile(f'ACDC/database/testing/patient{i}/patient{i}_frame{frame_num:02}_gt.nii.gz'):\n",
        "                patient_vols_test_gt.append(nib.load(f'ACDC/database/testing/patient{i}/patient{i}_frame{frame_num:02}_gt.nii.gz'))\n",
        "\n",
        "    return\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LqzJmU0ke1oI"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "yYDpQvxZe1oJ"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Data Preprocessing\n",
        "    def max_int_resize_and_normalize:\n",
        "    1. Convert to numpy array\n",
        "    2. Set max intenzity\n",
        "    3. Resize and normalize\n",
        "    4. Convert back to nifti format\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "def numpy2nifti(numpy_img):\n",
        "    return nib.Nifti1Image(numpy_img, original_affine)\n",
        "\n",
        "def nifti2numpy(nifti_img):\n",
        "    return nifti_img.get_fdata()\n",
        "\n",
        "def max_intenzity(image):\n",
        "    min_val = np.min(image)\n",
        "    max_val = np.max(image)\n",
        "    min_max_norm_img = ((image - min_val) / (max_val - min_val) * 255).astype(np.uint8)\n",
        "    return min_max_norm_img\n",
        "\n",
        "def normalize(image):\n",
        "    image = image.astype(np.float32)\n",
        "    image -= np.mean(image)\n",
        "    image /= np.std(image)\n",
        "    return numpy2nifti(image)\n",
        "\n",
        "# image and label\n",
        "def max_int_resize_and_normalize_(numpy_img, label):\n",
        "\n",
        "    resized_norm_data, resized_norm_label = skt.resize(max_intenzity(nifti2numpy(numpy_img)), (256,256,10), order=1, preserve_range=False,anti_aliasing=True), skt.resize(max_intenzity(nifti2numpy(label)), (256,256,10), order=1, preserve_range=False,anti_aliasing=True)\n",
        "    resized_norm_data, resized_norm_label = normalize(resized_norm_data), normalize(resized_norm_label)\n",
        "\n",
        "    return resized_norm_data, resized_norm_label\n",
        "\n",
        "# only image\n",
        "def max_int_resize_and_normalize(numpy_img):\n",
        "\n",
        "    resized_norm_data = skt.resize(max_intenzity(nifti2numpy(numpy_img)), (256,256,10), order=1, preserve_range=False,anti_aliasing=True)\n",
        "    resized_norm_data = normalize(resized_norm_data)\n",
        "\n",
        "    return resized_norm_data\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "Define the functions for the nifti data augmentation\n",
        "Every function get nifti image as input and return nifti image as output\n",
        "Save new nifti images to the database\n",
        "\n",
        "    1. Rotation\n",
        "    2. Random gaussian noise\n",
        "    3. Mirror around y axis\n",
        "    4. Mirror around x axis\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "def rotation_z_with_affine(image,rotation_degree=90):\n",
        "    rotation_radians = np.radians(rotation_degree)\n",
        "    rotation_affine = np.array([[np.cos(rotation_radians), -np.sin(rotation_radians), 0, 0],\n",
        "                                [np.sin(rotation_radians), np.cos(rotation_radians), 0, 0],\n",
        "                                [0, 0, 1, 0],\n",
        "                                [0, 0, 0, 1]])\n",
        "\n",
        "    affine_so_far = image.affine.dot(rotation_affine)\n",
        "    return nib.Nifti1Image(image, affine=affine_so_far)\n",
        "\n",
        "def rotation_z(nifti_img,rotation_degree):\n",
        "    img = nifti_img.get_fdata()\n",
        "    rotated_data = rotate(img, rotation_degree, (0, 1), reshape=False)\n",
        "    return nib.Nifti1Image(rotated_data, original_affine)\n",
        "\n",
        "def smooth(nifti_img, fwhm):\n",
        "    return nili.smooth_img(nifti_img, fwhm=fwhm)\n",
        "\n",
        "# affine change\n",
        "def mirror_y_with_affine(numpy_img):\n",
        "    return nib.Nifti1Image(numpy_img, np.eye(4))\n",
        "\n",
        "# npy data change\n",
        "def mirror_y(nifti_img):\n",
        "    mirrored_data = np.flip(nifti2numpy(nifti_img), axis=0)\n",
        "    return nib.Nifti1Image(mirrored_data, original_affine)\n",
        "\n",
        "# affine change\n",
        "def mirror_x_with_affine(numpy_img):\n",
        "    matrix = np.diag([-1, 1, 1, 1])\n",
        "    return nib.Nifti1Image(numpy_img, matrix)\n",
        "\n",
        "# npy data change\n",
        "def mirror_x(nifti_img):\n",
        "    mirrored_data = np.flip(nifti2numpy(nifti_img), axis=1)\n",
        "    return nib.Nifti1Image(mirrored_data, original_affine)\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "dataset, dataloader\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "def get_db_size(dataset):\n",
        "    print('Dataset size:', len(dataset), 'subjects')\n",
        "    return len(dataset)\n",
        "\n",
        "def create_dataloader(X, y, batch_size):\n",
        "    images = torch.Tensor(X)\n",
        "    ground_truths = torch.Tensor(y)\n",
        "    dataset = TensorDataset(X, y)\n",
        "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True) # shuffle the data\n",
        "\n",
        "    return dataloader\n",
        "def test_dataloader(X,batch_size):\n",
        "    test_images = torch.Tensor(X)\n",
        "    test_dataset = TensorDataset(test_images)\n",
        "    dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    return dataloader\n",
        "\n",
        "def eval(dataset, model):\n",
        "    model.eval()\n",
        "    with torch.no_grad():  # Kikapcsoljuk a gradiens számítást a tesztelés során\n",
        "        for inputs in dataset:\n",
        "            outputs = model(inputs)\n",
        "    return outputs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "_MM4zLsye1oJ"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "1. Read all the nifti images(train and test) and store them in lists\n",
        "2. Prepare the data for the network(reshape, normalize)\n",
        "3. Split the data to train, test and validation\n",
        "4. Data augmentation\n",
        "5. Save the normal and augmented data to npy files\n",
        "\n",
        "\"\"\"\n",
        "#Load the nifti files into lists(nifti format)\n",
        "load_nifti_files()\n",
        "\n",
        "    #Max intenzity,Resize and Normalize all images and ground truth images\n",
        "for i in range(len(patient_vols_train)):\n",
        "    patient_vols_train[i], patient_vols_train_gt[i] = max_int_resize_and_normalize_(patient_vols_train[i], patient_vols_train_gt[i])\n",
        "for i in range(len(patient_vols_test)):\n",
        "    patient_vols_test[i], patient_vols_test_gt[i] = max_int_resize_and_normalize_(patient_vols_test[i], patient_vols_test_gt[i])\n",
        "\n",
        "\n",
        "\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "RK45NLeqe1oJ"
      },
      "outputs": [],
      "source": [
        "# Data augmentation\n",
        "    # rotate and mirror the images, then add random noise to random images\n",
        "    # every train image has 2 rotated,2 mirrored and 1 noisy version\n",
        "    # make the same with the ground truth images\n",
        "    # 25*5 = 125\n",
        "augmented_img_train = []\n",
        "augmented_img_test = []\n",
        "augmented_gt_train = []\n",
        "augmented_gt_test = []\n",
        "np.random.seed(42)\n",
        "random_numbers = np.random.randint(0,200, size=25)\n",
        "\n",
        "for j in range(0,25):\n",
        "    i=random_numbers[j]\n",
        "    augmented_img_train.append(rotation_z(patient_vols_train[i], 90))\n",
        "    augmented_img_train.append(rotation_z(patient_vols_train[i], 270))\n",
        "    augmented_img_train.append(mirror_y(patient_vols_train[i]))\n",
        "    augmented_img_train.append(mirror_x(patient_vols_train[i]))\n",
        "    augmented_img_train.append(smooth(patient_vols_train[i], 2))\n",
        "    augmented_gt_train.append(rotation_z(patient_vols_train_gt[i], 90))\n",
        "    augmented_gt_train.append(rotation_z(patient_vols_train_gt[i], 270))\n",
        "    augmented_gt_train.append(mirror_y(patient_vols_train_gt[i]))\n",
        "    augmented_gt_train.append(mirror_x(patient_vols_train_gt[i]))\n",
        "    augmented_gt_train.append(smooth(patient_vols_train[i], 2))\n",
        "\n",
        "    # for the test images we make the same\n",
        "    # 10*5\n",
        "random_numbers = np.random.randint(0,100, size=10)\n",
        "for j in range(0,10):\n",
        "    i=random_numbers[j]\n",
        "    augmented_img_test.append(rotation_z(patient_vols_test[i], 90))\n",
        "    augmented_img_test.append(rotation_z(patient_vols_test[i], 270))\n",
        "    augmented_img_test.append(mirror_y(patient_vols_test[i]))\n",
        "    augmented_img_test.append(mirror_x(patient_vols_test[i]))\n",
        "    augmented_img_test.append(smooth(patient_vols_test[i],2))\n",
        "    augmented_gt_test.append(rotation_z(patient_vols_test_gt[i], 90))\n",
        "    augmented_gt_test.append(rotation_z(patient_vols_test_gt[i], 270))\n",
        "    augmented_gt_test.append(mirror_y(patient_vols_test_gt[i]))\n",
        "    augmented_gt_test.append(mirror_x(patient_vols_test_gt[i]))\n",
        "    augmented_gt_test.append(smooth(patient_vols_test_gt[i],2))\n",
        "\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "-7UHJtpue1oK"
      },
      "outputs": [],
      "source": [
        "X_train = []\n",
        "for img in patient_vols_train:\n",
        "    for i in range(1,img.shape[-1],2):\n",
        "        new_data = np.stack((img.get_fdata()[:,:,i],)*3, axis=-1).astype('float32')\n",
        "        X_train.append(new_data)\n",
        "\n",
        "X_train_aug = []\n",
        "for img in augmented_img_train:\n",
        "    for i in range(1,img.shape[-1],2):\n",
        "        new_data = np.stack((img.get_fdata()[:,:,i],)*3, axis=-1).astype('float32')\n",
        "        X_train_aug.append(new_data)\n",
        "\n",
        "\n",
        "Y_train = []\n",
        "for img in patient_vols_train_gt:\n",
        "    for i in range(1,img.shape[-1],2):\n",
        "        new_data = img.get_fdata()[:,:,i].astype('float32')\n",
        "        Y_train.append(new_data)\n",
        "Y_train_aug = []\n",
        "for img in augmented_gt_train:\n",
        "    for i in range(1,img.shape[-1],2):\n",
        "        new_data = img.get_fdata()[:,:,i].astype('float32')\n",
        "        Y_train_aug.append(new_data)\n",
        "\n",
        "X_test = []\n",
        "for img in patient_vols_test:\n",
        "    for i in range(1,img.shape[-1],2):\n",
        "        new_data = np.stack((img.get_fdata()[:,:,i],)*3, axis=-1).astype('float32')\n",
        "        X_test.append(new_data)\n",
        "X_test_aug = []\n",
        "for img in augmented_img_test:\n",
        "    for i in range(1,img.shape[-1],2):\n",
        "        new_data = np.stack((img.get_fdata()[:,:,i],)*3, axis=-1).astype('float32')\n",
        "        X_test_aug.append(new_data)\n",
        "\n",
        "y_test = []\n",
        "for img in patient_vols_test_gt:\n",
        "    for i in range(1,img.shape[-1],2):\n",
        "        new_data = img.get_fdata()[:,:,i].astype('float32')\n",
        "        y_test.append(new_data)\n",
        "y_test_aug = []\n",
        "for img in augmented_gt_test:\n",
        "    for i in range(1,img.shape[-1],2):\n",
        "        new_data = img.get_fdata()[:,:,i].astype('float32')\n",
        "        y_test_aug.append(new_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "ca3q2zXYe1oK"
      },
      "outputs": [],
      "source": [
        "X_train = np.array(X_train)\n",
        "X_train_aug = np.array(X_train_aug)\n",
        "Y_train = np.array(Y_train)\n",
        "Y_train_aug = np.array(Y_train_aug)\n",
        "X_test = np.array(X_test)\n",
        "X_test_aug = np.array(X_test_aug)\n",
        "y_test = np.array(y_test)\n",
        "y_test_aug = np.array(y_test_aug)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7E7eH0ZLe1oK",
        "outputId": "4ba4675d-4526-40d9-cf04-0a78bc66ac9f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_train shape: (1000, 256, 256, 3)\n",
            "X_train_aug shape: (625, 256, 256, 3)\n",
            "y_train shape: (1000, 256, 256)\n",
            "y_train_aug shape: (625, 256, 256)\n",
            "X_test shape: (500, 256, 256, 3)\n",
            "X_test_aug shape: (250, 256, 256, 3)\n",
            "y_test shape: (500, 256, 256)\n",
            "y_test_aug shape: (250, 256, 256)\n"
          ]
        }
      ],
      "source": [
        "print('X_train shape:', X_train.shape)\n",
        "print('X_train_aug shape:', X_train_aug.shape)\n",
        "print('y_train shape:', Y_train.shape)\n",
        "print('y_train_aug shape:', Y_train_aug.shape)\n",
        "print('X_test shape:', X_test.shape)\n",
        "print('X_test_aug shape:', X_test_aug.shape)\n",
        "print('y_test shape:', y_test.shape)\n",
        "print('y_test_aug shape:', y_test_aug.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "f3t8LA6fe1oL"
      },
      "outputs": [],
      "source": [
        "Y_train = np.expand_dims(Y_train, axis=3)\n",
        "Y_train_aug = np.expand_dims(Y_train_aug, axis=3)\n",
        "y_test = np.expand_dims(y_test, axis=3)\n",
        "y_test_aug = np.expand_dims(y_test_aug, axis=3)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H2aKDUq2e1oL",
        "outputId": "9c893c67-1cb0-4d55-ec28-708a90f9df2a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(1000, 256, 256, 3)\n",
            "(1000, 256, 256, 1)\n",
            "<class 'numpy.ndarray'>\n",
            "<class 'numpy.ndarray'>\n"
          ]
        }
      ],
      "source": [
        "print(X_train.shape)\n",
        "print(Y_train.shape)\n",
        "\n",
        "print(type(X_train))\n",
        "print(type(Y_train))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iGwWcitLe1oL",
        "outputId": "6c481870-088e-4188-8d25-700be7cc1212"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "absl-py==2.0.0\n",
            "asttokens==2.4.0\n",
            "astunparse==1.6.3\n",
            "backcall==0.2.0\n",
            "cachetools==5.3.1\n",
            "certifi==2023.7.22\n",
            "charset-normalizer==3.3.0\n",
            "click==8.1.7\n",
            "colorama==0.4.6\n",
            "comm==0.1.4\n",
            "contourpy==1.1.1\n",
            "cycler==0.12.1\n",
            "debugpy==1.8.0\n",
            "decorator==5.1.1\n",
            "Deprecated==1.2.14\n",
            "dicom2nifti==2.4.8\n",
            "dill==0.3.7\n",
            "efficientnet==1.0.0\n",
            "efficientnet-pytorch==0.7.1\n",
            "exceptiongroup==1.1.3\n",
            "executing==2.0.0\n",
            "filelock==3.12.4\n",
            "flatbuffers==23.5.26\n",
            "fonttools==4.43.1\n",
            "fsspec==2023.9.2\n",
            "gast==0.5.4\n",
            "google-auth==2.23.3\n",
            "google-auth-oauthlib==1.0.0\n",
            "google-pasta==0.2.0\n",
            "grpcio==1.59.0\n",
            "h5py==3.10.0\n",
            "huggingface-hub==0.18.0\n",
            "humanize==4.8.0\n",
            "idna==3.4\n",
            "image-classifiers==1.0.0\n",
            "imageio==2.31.5\n",
            "importlib-metadata==6.8.0\n",
            "importlib-resources==6.1.0\n",
            "ipykernel==6.25.2\n",
            "ipython==8.16.1\n",
            "jedi==0.19.1\n",
            "Jinja2==3.1.2\n",
            "joblib==1.3.2\n",
            "jupyter_client==8.4.0\n",
            "jupyter_core==5.4.0\n",
            "keras==2.14.0\n",
            "Keras-Applications==1.0.8\n",
            "kiwisolver==1.4.5\n",
            "lazy_loader==0.3\n",
            "libclang==16.0.6\n",
            "lxml==4.9.3\n",
            "Markdown==3.5\n",
            "markdown-it-py==3.0.0\n",
            "MarkupSafe==2.1.3\n",
            "matplotlib==3.8.0\n",
            "matplotlib-inline==0.1.6\n",
            "mdurl==0.1.2\n",
            "ml-dtypes==0.2.0\n",
            "motmetrics==1.4.0\n",
            "mpmath==1.3.0\n",
            "munch==4.0.0\n",
            "nest-asyncio==1.5.8\n",
            "networkx==3.1\n",
            "nibabel==5.1.0\n",
            "nilearn==0.10.2\n",
            "numpy==1.24.3\n",
            "oauthlib==3.2.2\n",
            "opencv-contrib-python==4.8.1.78\n",
            "opencv-python==4.8.1.78\n",
            "opt-einsum==3.3.0\n",
            "packaging==23.2\n",
            "pandas==2.1.1\n",
            "parso==0.8.3\n",
            "pascal-voc-writer==0.1.4\n",
            "patsy==0.5.3\n",
            "pickleshare==0.7.5\n",
            "Pillow==10.1.0\n",
            "platformdirs==3.11.0\n",
            "pretrainedmodels==0.7.4\n",
            "prompt-toolkit==3.0.39\n",
            "protobuf==4.24.4\n",
            "psutil==5.9.6\n",
            "pure-eval==0.2.2\n",
            "py-cpuinfo==9.0.0\n",
            "pyasn1==0.5.0\n",
            "pyasn1-modules==0.3.0\n",
            "pydicom==2.4.3\n",
            "pyfunctional==1.4.3\n",
            "Pygments==2.16.1\n",
            "pyparsing==3.1.1\n",
            "python-dateutil==2.8.2\n",
            "python-gdcm==3.0.22\n",
            "pytz==2023.3.post1\n",
            "pywin32==306\n",
            "PyYAML==6.0.1\n",
            "pyzmq==25.1.1\n",
            "requests==2.31.0\n",
            "requests-oauthlib==1.3.1\n",
            "rich==13.6.0\n",
            "rsa==4.9\n",
            "safetensors==0.4.0\n",
            "scikit-image==0.22.0\n",
            "scikit-learn==1.3.1\n",
            "scipy==1.10.1\n",
            "seaborn==0.13.0\n",
            "segmentation-models==1.0.1\n",
            "segmentation-models-pytorch==0.3.3\n",
            "shellingham==1.5.3\n",
            "SimpleITK==2.3.0\n",
            "six==1.16.0\n",
            "sklearn-utils==0.0.15\n",
            "stack-data==0.6.3\n",
            "statsmodels==0.14.0\n",
            "sympy==1.12\n",
            "tabulate==0.9.0\n",
            "tensorboard==2.14.1\n",
            "tensorboard-data-server==0.7.1\n",
            "tensorflow==2.14.0\n",
            "tensorflow-estimator==2.14.0\n",
            "tensorflow-intel==2.14.0\n",
            "tensorflow-io-gcs-filesystem==0.31.0\n",
            "termcolor==2.3.0\n",
            "thop==0.1.1.post2209072238\n",
            "threadpoolctl==3.2.0\n",
            "tifffile==2023.9.26\n",
            "timm==0.9.2\n",
            "torch==2.0.1\n",
            "torchio==0.19.1\n",
            "torchvision==0.16.0\n",
            "tornado==6.3.3\n",
            "tqdm==4.66.1\n",
            "traitlets==5.11.2\n",
            "typer==0.9.0\n",
            "typing_extensions==4.8.0\n",
            "tzdata==2023.3\n",
            "ultralytics==8.0.200\n",
            "urllib3==2.0.7\n",
            "wcwidth==0.2.8\n",
            "Werkzeug==3.0.0\n",
            "wrapt==1.14.1\n",
            "xmltodict==0.13.0\n",
            "zipp==3.17.0\n"
          ]
        }
      ],
      "source": [
        "!python -m pip freeze"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "fyed0eQbe1oL"
      },
      "outputs": [],
      "source": [
        "# save npy files\n",
        "\n",
        "\n",
        "np.save('X_train.npy', X_train)\n",
        "np.save('Y_train.npy', Y_train)\n",
        "np.save('X_train_aug.npy', X_train_aug)\n",
        "np.save('Y_train_aug.npy', Y_train_aug)\n",
        "np.save('X_test.npy', X_test)\n",
        "np.save('y_test.npy', y_test)\n",
        "np.save('X_test_aug.npy', X_test_aug)\n",
        "np.save('y_test_aug.npy', y_test_aug)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.1"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}